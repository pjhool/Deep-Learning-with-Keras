{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Embedding.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pjhool/Deep-Learning-with-Keras/blob/master/Word_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "eEYej0sVXFAR",
        "colab_type": "code",
        "outputId": "8695f949-0529-4af0-c2cf-00f54b08c743",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import *\n",
        "from keras.preprocessing.sequence import skipgrams\n",
        "\n",
        "text = \"I love green eggs and ham .\"\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "\n",
        "word2id = tokenizer.word_index\n",
        "id2word = {v:k for k, v in word2id.items()}\n",
        "\n",
        "wids = [word2id[w] for w in text_to_word_sequence(text)]\n",
        "pairs, labels = skipgrams(wids, len(word2id))\n",
        "print(len(pairs), len(labels))\n",
        "for i in range(10):\n",
        "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
        "        id2word[pairs[i][0]], pairs[i][0], \n",
        "        id2word[pairs[i][1]], pairs[i][1],\n",
        "        labels[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "56 56\n",
            "(love (2), green (3)) -> 1\n",
            "(i (1), i (1)) -> 0\n",
            "(love (2), eggs (4)) -> 0\n",
            "(green (3), green (3)) -> 0\n",
            "(i (1), eggs (4)) -> 1\n",
            "(green (3), green (3)) -> 0\n",
            "(and (5), and (5)) -> 0\n",
            "(ham (6), love (2)) -> 1\n",
            "(eggs (4), and (5)) -> 0\n",
            "(and (5), ham (6)) -> 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B_yZyU1YXfgD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# visualize Word Embedding \n",
        "\n",
        "# https://github.com/dennybritz/deeplearning-papernotes/blob/master/notes/document-embedding-with-pv.md \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ynWDloyoYCNl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kGUbvZugYchH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "5a9c3502-d1e6-4821-c1dd-f695bf0153f4"
      },
      "cell_type": "code",
      "source": [
        "# download coprus \n",
        "! mkdir ../data \n",
        "!wget http://mattmahoney.net/dc/text8.zip   \n",
        "!unzip text8.zip \n",
        "!cp text8 ../data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-11 10:54:45--  http://mattmahoney.net/dc/text8.zip\n",
            "Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.75\n",
            "Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.75|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31344016 (30M) [application/zip]\n",
            "Saving to: ‘text8.zip.1’\n",
            "\n",
            "text8.zip.1         100%[===================>]  29.89M  2.18MB/s    in 14s     \n",
            "\n",
            "2019-03-11 10:54:59 (2.16 MB/s) - ‘text8.zip.1’ saved [31344016/31344016]\n",
            "\n",
            "Archive:  text8.zip\n",
            "  inflating: text8                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AGGSRxl1ZLqq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "56680163-e405-4d00-a76a-de32f0336a22"
      },
      "cell_type": "code",
      "source": [
        "!ls -al ../data"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 97668\n",
            "drwxr-xr-x 2 root root      4096 Mar 11 10:55 .\n",
            "drwxr-xr-x 1 root root      4096 Mar 11 10:54 ..\n",
            "-rw-r--r-- 1 root root 100000000 Mar 11 10:55 text8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Zi8_9pcXYPwT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1038
        },
        "outputId": "c8423884-af90-4ebc-8a49-7548476a7af5"
      },
      "cell_type": "code",
      "source": [
        "from gensim.models import word2vec\n",
        "import os\n",
        "import logging\n",
        "\n",
        "class Text8Sentences(object):\n",
        "    def __init__(self, fname, maxlen):\n",
        "        self.fname = fname\n",
        "        self.maxlen = maxlen\n",
        "        \n",
        "    def __iter__(self):\n",
        "        with open(os.path.join(DATA_DIR, \"text8\"), \"rb\") as ftext:\n",
        "            text = ftext.read().split(\" \")\n",
        "            words = []\n",
        "            for word in text:\n",
        "                if len(words) >= self.maxlen:\n",
        "                    yield words\n",
        "                    words = []\n",
        "                words.append(word)\n",
        "            yield words\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "DATA_DIR = \"../data/\"\n",
        "sentences = Text8Sentences(os.path.join(DATA_DIR, \"text8\"), 50)\n",
        "model = word2vec.Word2Vec(sentences, size=300, min_count=30)\n",
        "\n",
        "print(\"\"\"model.most_similar(\"woman\")\"\"\")\n",
        "print(model.most_similar(\"woman\"))\n",
        "#[('child', 0.7057571411132812),\n",
        "# ('girl', 0.702182412147522),\n",
        "# ('man', 0.6846336126327515),\n",
        "# ('herself', 0.6292711496353149),\n",
        "# ('lady', 0.6229539513587952),\n",
        "# ('person', 0.6190367937088013),\n",
        "# ('lover', 0.6062309741973877),\n",
        "# ('baby', 0.5993420481681824),\n",
        "# ('mother', 0.5954475402832031),\n",
        "# ('daughter', 0.5871444940567017)]\n",
        " \n",
        "print(\"\"\"model.most_similar(positive=[\"woman\", \"king\"], negative=[\"man\"], topn=10)\"\"\")\n",
        "print(model.most_similar(positive=['woman', 'king'], \n",
        "                         negative=['man'], \n",
        "                         topn=10))\n",
        "#[('queen', 0.6237582564353943),\n",
        "# ('prince', 0.5638638734817505),\n",
        "# ('elizabeth', 0.5557916164398193),\n",
        "# ('princess', 0.5456407070159912),\n",
        "# ('throne', 0.5439794063568115),\n",
        "# ('daughter', 0.5364126563072205),\n",
        "# ('empress', 0.5354889631271362),\n",
        "# ('isabella', 0.5233952403068542),\n",
        "# ('regent', 0.520746111869812),\n",
        "# ('matilda', 0.5167444944381714)]                         \n",
        "                         \n",
        "print(\"\"\"model.similarity(\"girl\", \"woman\")\"\"\")\n",
        "print(model.similarity(\"girl\", \"woman\"))\n",
        "print(\"\"\"model.similarity(\"girl\", \"man\")\"\"\")\n",
        "print(model.similarity(\"girl\", \"man\"))\n",
        "print(\"\"\"model.similarity(\"girl\", \"car\")\"\"\")\n",
        "print(model.similarity(\"girl\", \"car\"))\n",
        "print(\"\"\"model.similarity(\"bus\", \"car\")\"\"\")\n",
        "print(model.similarity(\"bus\", \"car\"))\n",
        "#model.similarity(\"girl\", \"woman\")\n",
        "#0.702182479574\n",
        "#model.similarity(\"girl\", \"man\")\n",
        "#0.574259909834\n",
        "#model.similarity(\"girl\", \"car\")\n",
        "#0.289332921793\n",
        "#model.similarity(\"bus\", \"car\")\n",
        "#0.483853497748"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-03-11 10:55:26,986 : INFO : collecting all words and their counts\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ad0dcae778d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mDATA_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../data/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mText8Sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"model.most_similar(\"woman\")\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the sentences argument. Try an iterator.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m             self.train(\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \"\"\"\n\u001b[1;32m    935\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[0;32m--> 936\u001b[0;31m             sentences=sentences, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[0m\u001b[1;32m    937\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, sentences, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[1;32m   1569\u001b[0m             \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLineSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1571\u001b[0;31m         \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m         logger.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1538\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m         \u001b[0mchecked_string_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-ad0dcae778d0>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mftext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mftext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "V66aM9GuamUt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# dataset  Kaggle UMICH SI650 - Sentiment Classification \n",
        "#   https://www.kaggle.com/c/si650winter11 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K2YVE2i_ZxKk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "from keras.layers.core import Dense, Dropout, SpatialDropout1D\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.pooling import GlobalMaxPooling1D\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import train_test_split\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "INPUT_FILE = \"../data/umich-sentiment-train.txt\"\n",
        "VOCAB_SIZE = 5000\n",
        "EMBED_SIZE = 100\n",
        "NUM_FILTERS = 256\n",
        "NUM_WORDS = 3\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "counter = collections.Counter()\n",
        "fin = open(INPUT_FILE, \"rb\")\n",
        "maxlen = 0\n",
        "for line in fin:\n",
        "    _, sent = line.strip().split(\"\\t\")\n",
        "    words = [x.lower() for x in nltk.word_tokenize(sent)]\n",
        "    if len(words) > maxlen:\n",
        "        maxlen = len(words)\n",
        "    for word in words:\n",
        "        counter[word] += 1\n",
        "fin.close()\n",
        "\n",
        "word2index = collections.defaultdict(int)\n",
        "for wid, word in enumerate(counter.most_common(VOCAB_SIZE)):\n",
        "    word2index[word[0]] = wid + 1\n",
        "vocab_sz = len(word2index) + 1\n",
        "index2word = {v:k for k, v in word2index.items()}\n",
        "    \n",
        "xs, ys = [], []\n",
        "fin = open(INPUT_FILE, \"rb\")\n",
        "for line in fin:\n",
        "    label, sent = line.strip().split(\"\\t\")\n",
        "    ys.append(int(label))\n",
        "    words = [x.lower() for x in nltk.word_tokenize(sent)]\n",
        "    wids = [word2index[word] for word in words]\n",
        "    xs.append(wids)\n",
        "fin.close()\n",
        "X = pad_sequences(xs, maxlen=maxlen)\n",
        "Y = np_utils.to_categorical(ys)\n",
        "\n",
        "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3, \n",
        "                                                random_state=42)\n",
        "print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_sz, EMBED_SIZE, input_length=maxlen))\n",
        "model.add(SpatialDropout1D(Dropout(0.2)))\n",
        "model.add(Conv1D(filters=NUM_FILTERS, kernel_size=NUM_WORDS, activation=\"relu\"))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(2, activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,\n",
        "                    epochs=NUM_EPOCHS,\n",
        "                    validation_data=(Xtest, Ytest))              \n",
        "\n",
        "# plot loss function\n",
        "plt.subplot(211)\n",
        "plt.title(\"accuracy\")\n",
        "plt.plot(history.history[\"acc\"], color=\"r\", label=\"train\")\n",
        "plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"validation\")\n",
        "plt.legend(loc=\"best\")\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.title(\"loss\")\n",
        "plt.plot(history.history[\"loss\"], color=\"r\", label=\"train\")\n",
        "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"validation\")\n",
        "plt.legend(loc=\"best\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# evaluate model\n",
        "score = model.evaluate(Xtest, Ytest, verbose=1)\n",
        "print(\"Test score: {:.3f}, accuracy: {:.3f}\".format(score[0], score[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}