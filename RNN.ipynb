{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pjhool/Deep-Learning-with-Keras/blob/master/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "nk8VIZbSeTZN",
        "colab_type": "code",
        "outputId": "4e2bea82-bd16-4890-c83c-e6016c93dfd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "cell_type": "code",
      "source": [
        "# Dataset : http://www.gutenberg.org/files/11/11-0.txt\n",
        "! mkdir ../data \n",
        "\n",
        "!wget http://www.gutenberg.org/files/11/11-0.txt -O  ../data/alice_in_wonderland.txt "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-11 11:23:46--  http://www.gutenberg.org/files/11/11-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 173595 (170K) [text/plain]\n",
            "Saving to: ‘../data/alice_in_wonderland.txt’\n",
            "\n",
            "../data/alice_in_wo 100%[===================>] 169.53K   389KB/s    in 0.4s    \n",
            "\n",
            "2019-03-11 11:23:47 (389 KB/s) - ‘../data/alice_in_wonderland.txt’ saved [173595/173595]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "USnVnoUyfSTj",
        "colab_type": "code",
        "outputId": "5addeb63-a5e3-4e8d-b7cd-4208522d60ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2917
        }
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# Adapted from lstm_text_generation.py in keras/examples\n",
        "from __future__ import print_function\n",
        "from keras.layers.recurrent import SimpleRNN\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "import numpy as np\n",
        "\n",
        "INPUT_FILE = \"../data/alice_in_wonderland.txt\"\n",
        "\n",
        "# extract the input as a stream of characters\n",
        "print(\"Extracting text from input...\")\n",
        "fin = open(INPUT_FILE, 'rb')\n",
        "lines = []\n",
        "for line in fin:\n",
        "    line = line.strip().lower()\n",
        "    line = line.decode(\"ascii\", \"ignore\")\n",
        "    if len(line) == 0:\n",
        "        continue\n",
        "    lines.append(line)\n",
        "fin.close()\n",
        "text = \" \".join(lines)\n",
        "\n",
        "# creating lookup tables\n",
        "# Here chars is the number of features in our character \"vocabulary\"\n",
        "chars = set([c for c in text])\n",
        "nb_chars = len(chars)\n",
        "char2index = dict((c, i) for i, c in enumerate(chars))\n",
        "index2char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# create inputs and labels from the text. We do this by stepping\n",
        "# through the text ${step} character at a time, and extracting a \n",
        "# sequence of size ${seqlen} and the next output char. For example,\n",
        "# assuming an input text \"The sky was falling\", we would get the \n",
        "# following sequence of input_chars and label_chars (first 5 only)\n",
        "#   The sky wa -> s\n",
        "#   he sky was ->  \n",
        "#   e sky was  -> f\n",
        "#    sky was f -> a\n",
        "#   sky was fa -> l\n",
        "print(\"Creating input and label text...\")\n",
        "SEQLEN = 10\n",
        "STEP = 1\n",
        "\n",
        "input_chars = []\n",
        "label_chars = []\n",
        "for i in range(0, len(text) - SEQLEN, STEP):\n",
        "    input_chars.append(text[i:i + SEQLEN])\n",
        "    label_chars.append(text[i + SEQLEN])\n",
        "\n",
        "# vectorize the input and label chars\n",
        "# Each row of the input is represented by seqlen characters, each \n",
        "# represented as a 1-hot encoding of size len(char). There are \n",
        "# len(input_chars) such rows, so shape(X) is (len(input_chars),\n",
        "# seqlen, nb_chars).\n",
        "# Each row of output is a single character, also represented as a\n",
        "# dense encoding of size len(char). Hence shape(y) is (len(input_chars),\n",
        "# nb_chars).\n",
        "print(\"Vectorizing input and label text...\")\n",
        "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
        "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
        "for i, input_char in enumerate(input_chars):\n",
        "    for j, ch in enumerate(input_char):\n",
        "        X[i, j, char2index[ch]] = 1\n",
        "    y[i, char2index[label_chars[i]]] = 1\n",
        "\n",
        "# Build the model. We use a single RNN with a fully connected layer\n",
        "# to compute the most likely predicted output char\n",
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 128\n",
        "NUM_ITERATIONS = 25\n",
        "NUM_EPOCHS_PER_ITERATION = 1\n",
        "NUM_PREDS_PER_EPOCH = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(HIDDEN_SIZE, return_sequences=False,\n",
        "                    input_shape=(SEQLEN, nb_chars),\n",
        "                    unroll=True))\n",
        "model.add(Dense(nb_chars))\n",
        "model.add(Activation(\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
        "\n",
        "# We train the model in batches and test output generated at each step\n",
        "for iteration in range(NUM_ITERATIONS):\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Iteration #: %d\" % (iteration))\n",
        "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
        "    \n",
        "    # testing model\n",
        "    # randomly choose a row from input_chars, then use it to \n",
        "    # generate text from model for next 100 chars\n",
        "    test_idx = np.random.randint(len(input_chars))\n",
        "    test_chars = input_chars[test_idx]\n",
        "    print(\"Generating from seed: %s\" % (test_chars))\n",
        "    print(test_chars, end=\"\")\n",
        "    for i in range(NUM_PREDS_PER_EPOCH):\n",
        "        Xtest = np.zeros((1, SEQLEN, nb_chars))\n",
        "        for i, ch in enumerate(test_chars):\n",
        "            Xtest[0, i, char2index[ch]] = 1\n",
        "        pred = model.predict(Xtest, verbose=0)[0]\n",
        "        ypred = index2char[np.argmax(pred)]\n",
        "        print(ypred, end=\"\")\n",
        "        # move forward with test_chars + ypred\n",
        "        test_chars = test_chars[1:] + ypred\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting text from input...\n",
            "Creating input and label text...\n",
            "Vectorizing input and label text...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "==================================================\n",
            "Iteration #: 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 13s 85us/step - loss: 2.3391\n",
            "Generating from seed: the player\n",
            "the player the sard and the the sard and the the sard and the the sard and the the sard and the the sard and t\n",
            "==================================================\n",
            "Iteration #: 1\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 13s 79us/step - loss: 2.0501\n",
            "Generating from seed:  please do\n",
            " please do the was she was she was she was she was she was she was she was she was she was she was she was she\n",
            "==================================================\n",
            "Iteration #: 2\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 13s 80us/step - loss: 1.9483\n",
            "Generating from seed: ng date: j\n",
            "ng date: jurt the said the king the rooked an and the mand the mand the mand the mand the mand the mand the ma\n",
            "==================================================\n",
            "Iteration #: 3\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 13s 80us/step - loss: 1.8676\n",
            "Generating from seed:  name alic\n",
            " name alice to the grome the moush the moush the moush the moush the moush the moush the moush the moush the m\n",
            "==================================================\n",
            "Iteration #: 4\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 13s 80us/step - loss: 1.8004\n",
            "Generating from seed: y much, sa\n",
            "y much, said the docken the dore was the dormerse for a could the king the dore was the dormerse for a could t\n",
            "==================================================\n",
            "Iteration #: 5\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 13s 81us/step - loss: 1.7444\n",
            "Generating from seed: t my shoul\n",
            "t my should the king the could and the mouse of the could and the mouse of the could and the mouse of the coul\n",
            "==================================================\n",
            "Iteration #: 6\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 13s 79us/step - loss: 1.6979\n",
            "Generating from seed:  and then \n",
            " and then the really to the grope the right it was the really to the grope the right it was the really to the \n",
            "==================================================\n",
            "Iteration #: 7\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 13s 79us/step - loss: 1.6585\n",
            "Generating from seed:  quite a l\n",
            " quite a latter the rabbit in a little the project gutenberg-tm elect of the rabbit in a little the project gu\n",
            "==================================================\n",
            "Iteration #: 8\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 13s 79us/step - loss: 1.6249\n",
            "Generating from seed:  but she h\n",
            " but she her house and the mouse to the gryphon a little she was to the gryphon a little she was to the grypho\n",
            "==================================================\n",
            "Iteration #: 9\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 12s 78us/step - loss: 1.5964\n",
            "Generating from seed:  a very go\n",
            " a very gone of the was a little the project gutenberg-tm electronic works and the was a little the project gu\n",
            "==================================================\n",
            "Iteration #: 10\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 12s 78us/step - loss: 1.5724\n",
            "Generating from seed: orgot you \n",
            "orgot you mouse of the gond no the cat read to the parted to the parted to the parted to the parted to the par\n",
            "==================================================\n",
            "Iteration #: 11\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 12s 78us/step - loss: 1.5501\n",
            "Generating from seed: nce. what \n",
            "nce. what it was any sard alice to the caterpillar beantron, said the caterpillar beantron, said the caterpill\n",
            "==================================================\n",
            "Iteration #: 12\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 12s 78us/step - loss: 1.5304\n",
            "Generating from seed: ebook is f\n",
            "ebook is for the sore the room to the project gutenberg-tm electronic works to herself, it was a little the fo\n",
            "==================================================\n",
            "Iteration #: 13\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 12s 77us/step - loss: 1.5144\n",
            "Generating from seed: ed, perfor\n",
            "ed, perfort the soop as the way in the works to herself and said to herself and said to herself and said to he\n",
            "==================================================\n",
            "Iteration #: 14\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 13s 85us/step - loss: 1.4992\n",
            "Generating from seed:  she gave \n",
            " she gave the sorto it would the poor little thing in the poor little thing in the poor little thing in the po\n",
            "==================================================\n",
            "Iteration #: 15\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 12s 79us/step - loss: 1.4850\n",
            "Generating from seed: e lory, wi\n",
            "e lory, with an and dont of the rabbit in a little gring the rabbit in a little gring the rabbit in a little g\n",
            "==================================================\n",
            "Iteration #: 16\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 13s 82us/step - loss: 1.4715\n",
            "Generating from seed:  time! per\n",
            " time! personing at the door alice was so miden it was the dormouse in the project gutenberg-tm electronic wor\n",
            "==================================================\n",
            "Iteration #: 17\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 12s 78us/step - loss: 1.4606\n",
            "Generating from seed:  water out\n",
            " water out a the rabbit and the project gutenberg-tm electronic works in a coment to alice and her should be a\n",
            "==================================================\n",
            "Iteration #: 18\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 12s 78us/step - loss: 1.4505\n",
            "Generating from seed: ttached fu\n",
            "ttached fure and began beand, and the mouse to her said the caterpillar for the room, and she had not to be a \n",
            "==================================================\n",
            "Iteration #: 19\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 12s 78us/step - loss: 1.4396\n",
            "Generating from seed: w can i ha\n",
            "w can i have to her head to see in the words a little was to see you had not as she was to see you had not as \n",
            "==================================================\n",
            "Iteration #: 20\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 12s 76us/step - loss: 1.4308\n",
            "Generating from seed:  from this\n",
            " from this again, and alice as she was to the toor all the thant for the dormouse for the the the court in the\n",
            "==================================================\n",
            "Iteration #: 21\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 12s 78us/step - loss: 1.4232\n",
            "Generating from seed:  to my jaw\n",
            " to my jaw and the dormouse it in a little shart she was the march hare in a little shart she was the march ha\n",
            "==================================================\n",
            "Iteration #: 22\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 13s 79us/step - loss: 1.4152\n",
            "Generating from seed:  or of any\n",
            " or of any rather all the reass of the chere see she had began bean be whol terms a long alice as she had bega\n",
            "==================================================\n",
            "Iteration #: 23\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 13s 79us/step - loss: 1.4076\n",
            "Generating from seed:  sharing p\n",
            " sharing porions work on the little grave the rabbit the white rabbit and the more that she had not a could no\n",
            "==================================================\n",
            "Iteration #: 24\n",
            "Epoch 1/1\n",
            "158773/158773 [==============================] - 13s 80us/step - loss: 1.4000\n",
            "Generating from seed: had disapp\n",
            "had disapper and she said the caterpillar how and her she some of the sort a should be a parrence to herself a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_fzsgecDjaZQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "npIWKR_Nf73N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#  Stock Market Prediction \n",
        "\n",
        "# http://cs229.stanford.edu/proj2012/BernalFokPidaparthi-FinancialMarketTimeSeriesPredictionwithRecurrentNeural.pdf \n",
        "\n",
        "\n",
        "#  Classic Music\n",
        "\n",
        "# https://arxiv.org/abs/1612.01010 \n",
        "\n",
        "# Andrej Karpathy \n",
        "\n",
        "# http://karpathy.github.io/2015/05/21/rnn-effectiveness/ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "22RrO9SHho34",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NGc6kv76hqP-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "감성분석 "
      ]
    },
    {
      "metadata": {
        "id": "jaf7_WvGg153",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "from keras.layers.core import Activation, Dense, Dropout, SpatialDropout1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "DATA_DIR = \"../data\"\n",
        "\n",
        "MAX_FEATURES = 2000\n",
        "MAX_SENTENCE_LENGTH = 40\n",
        "\n",
        "EMBEDDING_SIZE = 128\n",
        "HIDDEN_LAYER_SIZE = 64\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Read training data and generate vocabulary\n",
        "maxlen = 0\n",
        "word_freqs = collections.Counter()\n",
        "num_recs = 0\n",
        "ftrain = open(os.path.join(DATA_DIR, \"umich-sentiment-train.txt\"), 'rb')\n",
        "for line in ftrain:\n",
        "    label, sentence = line.strip().split(\"\\t\")\n",
        "    words = nltk.word_tokenize(sentence.decode(\"ascii\", \"ignore\").lower())\n",
        "    if len(words) > maxlen:\n",
        "        maxlen = len(words)\n",
        "    for word in words:\n",
        "        word_freqs[word] += 1\n",
        "    num_recs += 1\n",
        "ftrain.close()\n",
        "\n",
        "## Get some information about our corpus\n",
        "#print maxlen            # 42\n",
        "#print len(word_freqs)   # 2313\n",
        "\n",
        "# 1 is UNK, 0 is PAD\n",
        "# We take MAX_FEATURES-1 featurs to accound for PAD\n",
        "vocab_size = min(MAX_FEATURES, len(word_freqs)) + 2\n",
        "word2index = {x[0]: i+2 for i, x in \n",
        "                enumerate(word_freqs.most_common(MAX_FEATURES))}\n",
        "word2index[\"PAD\"] = 0\n",
        "word2index[\"UNK\"] = 1\n",
        "index2word = {v:k for k, v in word2index.items()}\n",
        "\n",
        "# convert sentences to sequences\n",
        "X = np.empty((num_recs, ), dtype=list)\n",
        "y = np.zeros((num_recs, ))\n",
        "i = 0\n",
        "ftrain = open(os.path.join(DATA_DIR, \"umich-sentiment-train.txt\"), 'rb')\n",
        "for line in ftrain:\n",
        "    label, sentence = line.strip().split(\"\\t\")\n",
        "    words = nltk.word_tokenize(sentence.decode(\"ascii\", \"ignore\").lower())\n",
        "    seqs = []\n",
        "    for word in words:\n",
        "        if word2index.has_key(word):\n",
        "            seqs.append(word2index[word])\n",
        "        else:\n",
        "            seqs.append(word2index[\"UNK\"])\n",
        "    X[i] = seqs\n",
        "    y[i] = int(label)\n",
        "    i += 1\n",
        "ftrain.close()\n",
        "\n",
        "# Pad the sequences (left padded with zeros)\n",
        "X = sequence.pad_sequences(X, maxlen=MAX_SENTENCE_LENGTH)\n",
        "\n",
        "# Split input into training and test\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, \n",
        "                                                random_state=42)\n",
        "print(Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape)\n",
        "\n",
        "# Build model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, EMBEDDING_SIZE, \n",
        "                    input_length=MAX_SENTENCE_LENGTH))\n",
        "model.add(SpatialDropout1D(Dropout(0.2)))\n",
        "model.add(LSTM(HIDDEN_LAYER_SIZE, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation(\"sigmoid\"))\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", \n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, \n",
        "                    epochs=NUM_EPOCHS,\n",
        "                    validation_data=(Xtest, ytest))\n",
        "\n",
        "# plot loss and accuracy\n",
        "plt.subplot(211)\n",
        "plt.title(\"Accuracy\")\n",
        "plt.plot(history.history[\"acc\"], color=\"g\", label=\"Train\")\n",
        "plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"Validation\")\n",
        "plt.legend(loc=\"best\")\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.title(\"Loss\")\n",
        "plt.plot(history.history[\"loss\"], color=\"g\", label=\"Train\")\n",
        "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"Validation\")\n",
        "plt.legend(loc=\"best\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# evaluate\n",
        "score, acc = model.evaluate(Xtest, ytest, batch_size=BATCH_SIZE)\n",
        "print(\"Test score: %.3f, accuracy: %.3f\" % (score, acc))\n",
        "\n",
        "for i in range(5):\n",
        "    idx = np.random.randint(len(Xtest))\n",
        "    xtest = Xtest[idx].reshape(1,40)\n",
        "    ylabel = ytest[idx]\n",
        "    ypred = model.predict(xtest)[0][0]\n",
        "    sent = \" \".join([index2word[x] for x in xtest[0].tolist() if x != 0])\n",
        "    print(\"%.0f\\t%d\\t%s\" % (ypred, ylabel, sent))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_OYp9aoKs6jw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 질의 응답 메모리 네트워크 \n",
        "# Facebook , bAbI Project \n",
        "\n",
        "# https://research.fb.com/downloads/babi/  \n",
        "\n",
        "# https://github.com/harvardnlp/MemN2N/tree/master/babi_data/en "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KnU0XoxAtLzR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import division, print_function\n",
        "from keras.layers import Input\n",
        "from keras.layers.core import Activation, Dense, Dropout, Permute\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.merge import add, concatenate, dot\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import np_utils\n",
        "import collections\n",
        "import itertools\n",
        "import nltk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def get_data(infile):\n",
        "    stories, questions, answers = [], [], []\n",
        "    story_text = []\n",
        "    fin = open(TRAIN_FILE, \"rb\")\n",
        "    for line in fin:\n",
        "        line = line.decode(\"utf-8\").strip()\n",
        "        lno, text = line.split(\" \", 1)\n",
        "        if \"\\t\" in text:\n",
        "            question, answer, _ = text.split(\"\\t\")\n",
        "            stories.append(story_text)\n",
        "            questions.append(question)\n",
        "            answers.append(answer)\n",
        "            story_text = []\n",
        "        else:\n",
        "            story_text.append(text)\n",
        "    fin.close()\n",
        "    return stories, questions, answers\n",
        "\n",
        "def build_vocab(train_data, test_data):\n",
        "    counter = collections.Counter()\n",
        "    for stories, questions, answers in [train_data, test_data]:\n",
        "        for story in stories:\n",
        "            for sent in story:\n",
        "                for word in nltk.word_tokenize(sent):\n",
        "                    counter[word.lower()] += 1\n",
        "        for question in questions:\n",
        "            for word in nltk.word_tokenize(question):\n",
        "                counter[word.lower()] += 1\n",
        "        for answer in answers:\n",
        "            for word in nltk.word_tokenize(answer):\n",
        "                counter[word.lower()] += 1\n",
        "    # no OOV here because there are not too many words in dataset\n",
        "    word2idx = {w:(i+1) for i, (w, _) in enumerate(counter.most_common())}\n",
        "    word2idx[\"PAD\"] = 0\n",
        "    idx2word = {v:k for k, v in word2idx.items()}\n",
        "    return word2idx, idx2word\n",
        "\n",
        "def get_maxlens(train_data, test_data):\n",
        "    story_maxlen, question_maxlen = 0, 0\n",
        "    for stories, questions, _ in [train_data, test_data]:\n",
        "        for story in stories:\n",
        "            story_len = 0\n",
        "            for sent in story:\n",
        "                swords = nltk.word_tokenize(sent)\n",
        "                story_len += len(swords)\n",
        "            if story_len > story_maxlen:\n",
        "                story_maxlen = story_len\n",
        "        for question in questions:\n",
        "            question_len = len(nltk.word_tokenize(question))\n",
        "            if question_len > question_maxlen:\n",
        "                question_maxlen = question_len\n",
        "    return story_maxlen, question_maxlen\n",
        "\n",
        "def vectorize(data, word2idx, story_maxlen, question_maxlen):\n",
        "    Xs, Xq, Y = [], [], []\n",
        "    stories, questions, answers = data\n",
        "    for story, question, answer in zip(stories, questions, answers):\n",
        "        xs = [[word2idx[w.lower()] for w in nltk.word_tokenize(s)] \n",
        "                                   for s in story]\n",
        "        xs = list(itertools.chain.from_iterable(xs))\n",
        "        xq = [word2idx[w.lower()] for w in nltk.word_tokenize(question)]\n",
        "        Xs.append(xs)\n",
        "        Xq.append(xq)\n",
        "        Y.append(word2idx[answer.lower()])\n",
        "    return pad_sequences(Xs, maxlen=story_maxlen),\\\n",
        "           pad_sequences(Xq, maxlen=question_maxlen),\\\n",
        "           np_utils.to_categorical(Y, num_classes=len(word2idx))\n",
        "\n",
        "\n",
        "DATA_DIR = \"../data\"\n",
        "\n",
        "TRAIN_FILE = os.path.join(DATA_DIR, \"qa1_single-supporting-fact_train.txt\")\n",
        "TEST_FILE = os.path.join(DATA_DIR, \"qa1_single-supporting-fact_test.txt\")\n",
        "\n",
        "# get the data\n",
        "data_train = get_data(TRAIN_FILE)\n",
        "data_test = get_data(TEST_FILE)\n",
        "\n",
        "print(len(data_train[0]), len(data_test[0]))\n",
        "\n",
        "# build vocabulary from all the data\n",
        "word2idx, idx2word = build_vocab(data_train, data_test)\n",
        "\n",
        "vocab_size = len(word2idx)\n",
        "print(\"vocab size: {:d}\".format(len(word2idx)))\n",
        "\n",
        "# compute max sequence length for each entity\n",
        "story_maxlen, question_maxlen = get_maxlens(data_train, data_test)\n",
        "print(\"story maxlen: {:d}, question maxlen: {:d}\".format(story_maxlen, question_maxlen))\n",
        "\n",
        "# vectorize the data\n",
        "Xstrain, Xqtrain, Ytrain = vectorize(data_train, word2idx, story_maxlen, question_maxlen)\n",
        "Xstest, Xqtest, Ytest = vectorize(data_test, word2idx, story_maxlen, question_maxlen)\n",
        "\n",
        "print(Xstrain.shape, Xqtrain.shape, Ytrain.shape, Xstest.shape, Xqtest.shape, Ytest.shape)\n",
        "\n",
        "# define network\n",
        "EMBEDDING_SIZE = 64\n",
        "LATENT_SIZE = 32\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# inputs\n",
        "story_input = Input(shape=(story_maxlen,))\n",
        "question_input = Input(shape=(question_maxlen,))\n",
        "\n",
        "# story encoder memory\n",
        "story_encoder = Embedding(input_dim=vocab_size,\n",
        "                         output_dim=EMBEDDING_SIZE,\n",
        "                         input_length=story_maxlen)(story_input)\n",
        "story_encoder = Dropout(0.3)(story_encoder)\n",
        "\n",
        "# question encoder\n",
        "question_encoder = Embedding(input_dim=vocab_size,\n",
        "                            output_dim=EMBEDDING_SIZE,\n",
        "                            input_length=question_maxlen)(question_input)\n",
        "question_encoder = Dropout(0.3)(question_encoder)\n",
        "\n",
        "# match between story and question\n",
        "match = dot([story_encoder, question_encoder], axes=[2, 2])\n",
        "\n",
        "# encode story into vector space of question\n",
        "story_encoder_c = Embedding(input_dim=vocab_size,\n",
        "                           output_dim=question_maxlen,\n",
        "                           input_length=story_maxlen)(story_input)\n",
        "story_encoder_c = Dropout(0.3)(story_encoder_c)\n",
        "\n",
        "# combine match and story vectors\n",
        "response = add([match, story_encoder_c])\n",
        "response = Permute((2, 1))(response)\n",
        "\n",
        "# combine response and question vectors\n",
        "answer = concatenate([response, question_encoder], axis=-1)\n",
        "answer = LSTM(LATENT_SIZE)(answer)\n",
        "answer = Dropout(0.3)(answer)\n",
        "answer = Dense(vocab_size)(answer)\n",
        "output = Activation(\"softmax\")(answer)\n",
        "\n",
        "model = Model(inputs=[story_input, question_input], outputs=output)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# train model\n",
        "history = model.fit([Xstrain, Xqtrain], [Ytrain], batch_size=BATCH_SIZE, \n",
        "                    epochs=NUM_EPOCHS,\n",
        "                    validation_data=([Xstest, Xqtest], [Ytest]))\n",
        "                    \n",
        "# plot accuracy and loss plot\n",
        "plt.subplot(211)\n",
        "plt.title(\"Accuracy\")\n",
        "plt.plot(history.history[\"acc\"], color=\"g\", label=\"train\")\n",
        "plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"validation\")\n",
        "plt.legend(loc=\"best\")\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.title(\"Loss\")\n",
        "plt.plot(history.history[\"loss\"], color=\"g\", label=\"train\")\n",
        "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"validation\")\n",
        "plt.legend(loc=\"best\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# labels\n",
        "ytest = np.argmax(Ytest, axis=1)\n",
        "\n",
        "# get predictions\n",
        "Ytest_ = model.predict([Xstest, Xqtest])\n",
        "ytest_ = np.argmax(Ytest_, axis=1)\n",
        "\n",
        "NUM_DISPLAY = 10\n",
        "\n",
        "for i in range(NUM_DISPLAY):\n",
        "    story = \" \".join([idx2word[x] for x in Xstest[i].tolist() if x != 0])\n",
        "    question = \" \".join([idx2word[x] for x in Xqtest[i].tolist()])\n",
        "    label = idx2word[ytest[i]]\n",
        "    prediction = idx2word[ytest_[i]]\n",
        "    print(story, question, label, prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k2SpKXMzjbom",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# UCI 머신러닝 저장소 \n",
        "# ElectricityLoadDiagrams20112014 \n",
        "\n",
        "# https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "njv9pcyGj3Ae",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DataSet Plot \n",
        "\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "from __future__ import division, print_function\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "\n",
        "DATA_DIR = \"../data\"\n",
        "\n",
        "fld = open(os.path.join(DATA_DIR, \"LD2011_2014.txt\"), \"rb\")\n",
        "data = []\n",
        "line_num = 0\n",
        "#cid = np.random.randint(0, 370, 1)\n",
        "cid = 250\n",
        "for line in fld:\n",
        "    if line.startswith(\"\\\"\\\";\"):\n",
        "        continue\n",
        "    if line_num % 100 == 0:\n",
        "        print(\"{:d} lines read\".format(line_num))\n",
        "    cols = [float(re.sub(\",\", \".\", x)) for x in \n",
        "            line.strip().split(\";\")[1:]]\n",
        "    data.append(cols[cid])\n",
        "    line_num += 1\n",
        "fld.close()\n",
        "\n",
        "NUM_ENTRIES = 1000\n",
        "plt.plot(range(NUM_ENTRIES), data[0:NUM_ENTRIES])\n",
        "plt.ylabel(\"electricity consumption\")\n",
        "plt.xlabel(\"time (1pt = 15 mins)\")\n",
        "plt.show()\n",
        "\n",
        "np.save(os.path.join(DATA_DIR, \"LD_250.npy\"), np.array(data))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dzpLJarpjcF-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from __future__ import division, print_function\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import Sequential\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "\n",
        "DATA_DIR = \"../data\"\n",
        "\n",
        "data = np.load(os.path.join(DATA_DIR, \"LD_250.npy\"))\n",
        "\n",
        "STATELESS = False\n",
        "\n",
        "NUM_TIMESTEPS = 20\n",
        "HIDDEN_SIZE = 10\n",
        "BATCH_SIZE = 96  # 24 hours (15 min intervals)\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# scale the data to be in the range (0, 1)\n",
        "data = data.reshape(-1, 1)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n",
        "data = scaler.fit_transform(data)\n",
        "\n",
        "# transform to 4 inputs -> 1 label format\n",
        "X = np.zeros((data.shape[0], NUM_TIMESTEPS))\n",
        "Y = np.zeros((data.shape[0], 1))\n",
        "for i in range(len(data) - NUM_TIMESTEPS - 1):\n",
        "    X[i] = data[i:i + NUM_TIMESTEPS].T\n",
        "    Y[i] = data[i + NUM_TIMESTEPS + 1]\n",
        "\n",
        "# reshape X to three dimensions (samples, timesteps, features)\n",
        "X = np.expand_dims(X, axis=2)\n",
        "\n",
        "# split into training and test sets (add the extra offsets so \n",
        "# we can use batch size of 5)\n",
        "sp = int(0.7 * len(data))\n",
        "Xtrain, Xtest, Ytrain, Ytest = X[0:sp], X[sp:], Y[0:sp], Y[sp:]\n",
        "print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)\n",
        "\n",
        "if STATELESS:\n",
        "    # stateless\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(HIDDEN_SIZE, input_shape=(NUM_TIMESTEPS, 1), \n",
        "                   return_sequences=False))\n",
        "    model.add(Dense(1))\n",
        "else:\n",
        "    # stateful\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(HIDDEN_SIZE, stateful=True,\n",
        "                   batch_input_shape=(BATCH_SIZE, NUM_TIMESTEPS, 1), \n",
        "                   return_sequences=False))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\",\n",
        "              metrics=[\"mean_squared_error\"])\n",
        "\n",
        "if STATELESS:\n",
        "    # stateless\n",
        "    model.fit(Xtrain, Ytrain, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,\n",
        "              validation_data=(Xtest, Ytest),\n",
        "              shuffle=False)\n",
        "else:\n",
        "    # stateful\n",
        "    # need to make training and test data to multiple of BATCH_SIZE\n",
        "    train_size = (Xtrain.shape[0] // BATCH_SIZE) * BATCH_SIZE\n",
        "    test_size = (Xtest.shape[0] // BATCH_SIZE) * BATCH_SIZE\n",
        "    Xtrain, Ytrain = Xtrain[0:train_size], Ytrain[0:train_size]\n",
        "    Xtest, Ytest = Xtest[0:test_size], Ytest[0:test_size]\n",
        "    print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)\n",
        "    for i in range(NUM_EPOCHS):\n",
        "        print(\"Epoch {:d}/{:d}\".format(i+1, NUM_EPOCHS))\n",
        "        model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE, epochs=1,\n",
        "                  validation_data=(Xtest, Ytest),\n",
        "                  shuffle=False)\n",
        "        model.reset_states()\n",
        "\n",
        "score, _ = model.evaluate(Xtest, Ytest, batch_size=BATCH_SIZE)\n",
        "rmse = math.sqrt(score)\n",
        "print(\"\\nMSE: {:.3f}, RMSE: {:.3f}\".format(score, rmse))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}